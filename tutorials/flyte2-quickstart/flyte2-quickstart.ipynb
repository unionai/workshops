{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae779f5",
   "metadata": {},
   "source": [
    "# Intro to Flyte 2\n",
    "\n",
    "To run this tutorial you'll need to be either an exsisting Union.ai user with Flyte v2, or [sign up for the free Beta access](https://www.union.ai/beta)!\n",
    "\n",
    "Flyte 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594d4eb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "if you're running this notebook locally we suggest creating a virtual environment and installing the packages locally with uv. You can follow the instructions on the README.\n",
    "```\n",
    "uv venv .venv --python=python3.11\n",
    "source .venv/bin/activate\n",
    "uv pip install flyte>=0.2.0b21 --prerelease=allow\n",
    "```\n",
    "If you're running this in google colab run the two setup cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f137f9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 0.2.0b21 not found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!uv pip install flyte>=0.2.0b21 --prerelease=allow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabef4a",
   "metadata": {},
   "source": [
    "##### Flyte Config\n",
    "if you're running this locally remove `--auth-type headless\\`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flyte create config \\\n",
    "    --endpoint demo.hosted.unionai.cloud \\\n",
    "    --auth-type headless\\\n",
    "    --builder remote \\\n",
    "    --domain development \\\n",
    "    --project flytesnacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d56d7",
   "metadata": {},
   "source": [
    "## ðŸ‘‹ Hello Flyte Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flyte\n",
    "\n",
    "env = flyte.TaskEnvironment(\n",
    "    name=\"hello_flyte_v2\",\n",
    "    resources=flyte.Resources(cpu=1, memory=\"250Mi\"),\n",
    ")\n",
    "\n",
    "\n",
    "@env.task\n",
    "def extract_features(user_input: str) -> dict:\n",
    "    clean = user_input.strip().lower()\n",
    "    return {\n",
    "        \"text\": clean,\n",
    "        \"length\": len(clean),\n",
    "        \"word_count\": len(clean.split()),\n",
    "        \"has_numbers\": any(c.isdigit() for c in clean)\n",
    "    }\n",
    "\n",
    "@env.task\n",
    "def validate_features(features: dict) -> bool:\n",
    "    return features[\"length\"] > 5 and features[\"word_count\"] >= 2\n",
    "\n",
    "@env.task\n",
    "def prepare_training_data(raw_inputs: list[str]) -> dict:\n",
    "    if len(raw_inputs) < 5:\n",
    "        raise ValueError(f\"Need at least 5 samples, got: {len(raw_inputs)}\")\n",
    "\n",
    "    features = list(flyte.map(extract_features, raw_inputs))\n",
    "    valid_flags = list(flyte.map(validate_features, features))\n",
    "\n",
    "    return {\n",
    "        \"total_samples\": len(features),\n",
    "        \"valid_samples\": sum(valid_flags),\n",
    "        \"avg_length\": sum(f[\"length\"] for f in features) / len(features),\n",
    "        \"ready_for_training\": sum(valid_flags) >= len(features) * 0.8\n",
    "    }\n",
    "\n",
    "# Sample user reviews/comments for sentiment analysis training\n",
    "sample_inputs = [\n",
    "    \"  This product is amazing! I love it.  \",    # valid: >5 chars, 2+ words\n",
    "    \"Great quality and fast shipping\",             # valid: >5 chars, 2+ words\n",
    "    \" Bad \",                                       # invalid: too short\n",
    "    \"okay\",                                        # invalid: <2 words\n",
    "    \"The delivery was delayed by 3 days\",         # valid: >5 chars, 2+ words, has numbers\n",
    "    \"Excellent customer service team\",             # valid: >5 chars, 2+ words\n",
    "    \"Perfect for my home office setup\",           # valid: >5 chars, 2+ words\n",
    "    \"meh\"                                          # invalid: too short, 1 word\n",
    "]\n",
    "\n",
    "flyte.init_from_config(\".flyte/config.yaml\")\n",
    "execution = flyte.run(prepare_training_data, raw_inputs=sample_inputs)\n",
    "print(f\"Execution: {execution.name}\")\n",
    "print(f\"URL: {execution.url}\")\n",
    "# Click on signin link to auth the first workflow ðŸ‘‡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd8f3d",
   "metadata": {},
   "source": [
    "#### Run tasks locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flyte.init() #overwrite flyte init from config\n",
    "execution = flyte.run(prepare_training_data, raw_inputs=sample_inputs)\n",
    "print(f\"Execution: {execution.name}\")\n",
    "print(f\"URL: {execution.url}\")\n",
    "#todo: output local info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115368d7",
   "metadata": {},
   "source": [
    "## Build an ML Pipeline (and see more features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ec169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flyte\n",
    "from flyte.io import Dir, File\n",
    "\n",
    "# Custom environment with scikit-learn installed\n",
    "env = flyte.TaskEnvironment(\n",
    "    name=\"scikit_learn\",\n",
    "    resources=flyte.Resources(cpu=1, memory=\"500Mi\"),\n",
    "    image=flyte.Image.from_debian_base().with_pip_packages(\"scikit-learn\",\n",
    "                                                           \"unionai-reuse==0.1.5\"),\n",
    "    reusable=flyte.ReusePolicy(\n",
    "        replicas=3,\n",
    "        idle_ttl=60,\n",
    "        concurrency=6,\n",
    "        scaledown_ttl=60,\n",
    "    ),\n",
    ")\n",
    "\n",
    "@env.task(cache=\"auto\")\n",
    "async def load_iris_data() -> tuple[File, File]:\n",
    "    \"\"\"Load the iris dataset, perform train-test split, and save to separate files.\"\"\"\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pickle\n",
    "\n",
    "    iris = load_iris()\n",
    "\n",
    "    # Perform train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Package training data\n",
    "    train_data = {\n",
    "        \"features\": X_train.tolist(),\n",
    "        \"targets\": y_train.tolist(),\n",
    "        \"feature_names\": list(iris.feature_names)\n",
    "    }\n",
    "\n",
    "    # Package test data\n",
    "    test_data = {\n",
    "        \"features\": X_test.tolist(),\n",
    "        \"targets\": y_test.tolist(),\n",
    "        \"feature_names\": list(iris.feature_names)\n",
    "    }\n",
    "\n",
    "    # Save training data to file\n",
    "    with open(\"train_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "\n",
    "    # Save test data to file\n",
    "    with open(\"test_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(test_data, f)\n",
    "\n",
    "    train_file = await File.from_local(\"train_data.pkl\")\n",
    "    test_file = await File.from_local(\"test_data.pkl\")\n",
    "\n",
    "    return train_file, test_file\n",
    "\n",
    "@env.task\n",
    "async def train_model(train_data_file: File) -> File:\n",
    "    \"\"\"Train a classifier using the training data.\"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pickle\n",
    "\n",
    "    # Load training data\n",
    "    async with train_data_file.open() as f:\n",
    "        if hasattr(f, 'read'):\n",
    "            content = f.read()\n",
    "        else:\n",
    "            content = await f.read()\n",
    "    train_data = pickle.loads(content)\n",
    "\n",
    "    X_train = train_data[\"features\"]\n",
    "    y_train = train_data[\"targets\"]\n",
    "\n",
    "    # Train model\n",
    "    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Package model with metadata\n",
    "    model_package = {\n",
    "        \"model\": clf,\n",
    "        \"feature_names\": train_data[\"feature_names\"],\n",
    "        \"train_samples\": len(X_train)\n",
    "    }\n",
    "\n",
    "    # Save model to file\n",
    "    with open(\"trained_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model_package, f)\n",
    "\n",
    "    model_file = await File.from_local(\"trained_model.pkl\")\n",
    "    return model_file\n",
    "\n",
    "@env.task\n",
    "async def validate_model(model_file: File, test_data_file: File) -> dict:\n",
    "    \"\"\"Validate the trained model using test data and return performance metrics.\"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import pickle\n",
    "\n",
    "    # Load trained model\n",
    "    async with model_file.open() as f:\n",
    "        if hasattr(f, 'read'):\n",
    "            model_content = f.read()\n",
    "        else:\n",
    "            model_content = await f.read()\n",
    "\n",
    "    if isinstance(model_content, str):\n",
    "        model_content = model_content.encode()\n",
    "\n",
    "    model_package = pickle.loads(model_content)\n",
    "    clf = model_package[\"model\"]\n",
    "\n",
    "    # Load test data\n",
    "    async with test_data_file.open() as f:\n",
    "        if hasattr(f, 'read'):\n",
    "            test_content = f.read()\n",
    "        else:\n",
    "            test_content = await f.read()\n",
    "\n",
    "    if isinstance(test_content, str):\n",
    "        test_content = test_content.encode()\n",
    "\n",
    "    test_data = pickle.loads(test_content)\n",
    "    X_test = test_data[\"features\"]\n",
    "    y_test = test_data[\"targets\"]\n",
    "\n",
    "    # Evaluate model on test data\n",
    "    test_predictions = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "    return {\n",
    "        \"model_type\": \"RandomForest\",\n",
    "        \"dataset\": \"iris\",\n",
    "        \"train_samples\": model_package[\"train_samples\"],\n",
    "        \"test_samples\": len(X_test),\n",
    "        \"test_accuracy\": round(test_acc, 3),\n",
    "        \"feature_importance\": {\n",
    "            name: round(importance, 3)\n",
    "            for name, importance in zip(model_package[\"feature_names\"], clf.feature_importances_)\n",
    "        }\n",
    "    }\n",
    "\n",
    "@env.task\n",
    "async def ml_pipeline() -> File:\n",
    "    \"\"\"Complete ML pipeline: data loading â†’ training â†’ validation.\"\"\"\n",
    "\n",
    "    # Step 1: Load data and perform train-test split\n",
    "    train_file, test_file = await load_iris_data()\n",
    "\n",
    "    # Step 2: Train model using training data\n",
    "    model_file = await train_model(train_file)\n",
    "\n",
    "    # Step 3: Validate model using test data\n",
    "    validation_results = await validate_model(model_file, test_file)\n",
    "\n",
    "    # Add pipeline metadata\n",
    "    validation_results[\"pipeline_status\"] = \"completed\"\n",
    "\n",
    "    return model_file\n",
    "\n",
    "# Main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    flyte.init_from_config(\".flyte/config.yaml\")\n",
    "    # flyte.init() # uncomment to run locally\n",
    "\n",
    "    # Run the complete pipeline\n",
    "    execution = flyte.run(ml_pipeline)\n",
    "\n",
    "    print(f\"Execution: {execution.name}\")\n",
    "    print(f\"URL: {execution.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea609fe",
   "metadata": {},
   "source": [
    "#### Outputs & Remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c331bc11",
   "metadata": {},
   "source": [
    "## âš ï¸ Error Handling & Dynamic Infrastructure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5647df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import flyte\n",
    "import flyte.errors\n",
    "\n",
    "env = flyte.TaskEnvironment(\n",
    "    name=\"fail\",\n",
    "    resources=flyte.Resources(cpu=1, memory=\"250Mi\"),\n",
    ")\n",
    "\n",
    "\n",
    "@env.task\n",
    "async def oomer(x: int):\n",
    "    large_list = [0] * 100000000\n",
    "    print(len(large_list))\n",
    "\n",
    "\n",
    "@env.task\n",
    "async def always_succeeds() -> int:\n",
    "    await asyncio.sleep(1)\n",
    "    return 42\n",
    "\n",
    "\n",
    "@env.task\n",
    "async def failure_recovery() -> int:\n",
    "    try:\n",
    "        await oomer(2)\n",
    "    except flyte.errors.OOMError as e:\n",
    "        print(f\"Failed with oom trying with more resources: {e}, of type {type(e)}, {e.code}\")\n",
    "        try:\n",
    "            await oomer.override(resources=flyte.Resources(cpu=1, memory=\"1Gi\"))(5)\n",
    "        except flyte.errors.OOMError as e:\n",
    "            print(f\"Failed with OOM Again giving up: {e}, of type {type(e)}, {e.code}\")\n",
    "            raise e\n",
    "    finally:\n",
    "        await always_succeeds()\n",
    "\n",
    "    return await always_succeeds()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    flyte.init_from_config(\".flyte/config.yaml\")\n",
    "\n",
    "    run = flyte.run(failure_recovery)\n",
    "    print(run.url)\n",
    "    run.wait(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263385b",
   "metadata": {},
   "source": [
    "## AI Agents & Agentic Workflows\n",
    "\n",
    "Flyte 2.0 built in dynamic task and workflows make it easy to build agentic workflows that can call LLMs and other AI models to help make decisions and take actions with out of the box support for most major agent frameworks and LLM providers.\n",
    "\n",
    "Example Coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5523b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
